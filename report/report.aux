\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{settles_active_2012}
\citation{settles_active_2012}
\citation{lewis_sequential_1994}
\citation{zhu_semi-supervised_2005}
\citation{goodfellow_deep_nodate}
\citation{settles_active_2012}
\citation{zhu_semi-supervised_2005}
\@writefile{toc}{\contentsline {section}{\numberline {1} Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2} Related Work}{1}{section.2}}
\newlabel{sec:related-work}{{2}{1}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3} Method}{1}{section.3}}
\newlabel{sec:method}{{3}{1}{Method}{section.3}{}}
\citation{krizhevsky_imagenet_2012}
\citation{nair_rectified_nodate}
\citation{kingma_auto-encoding_2013}
\citation{maaten_learning_2009}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces our initial query selection method (red) in relation to a downstream learning method (blue) like active or semi-supervised learning.}}{2}{figure.1}}
\newlabel{fig:overview}{{1}{2}{our initial query selection method (red) in relation to a downstream learning method (blue) like active or semi-supervised learning}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1} Auto-encoder Architectures}{2}{subsection.3.1}}
\newlabel{sec:auto-encod-arch}{{3.1}{2}{Auto-encoder Architectures}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1} Convolutional Auto-encoder}{2}{subsubsection.3.1.1}}
\newlabel{sec:conv-auto-encod}{{3.1.1}{2}{Convolutional Auto-encoder}{subsubsection.3.1.1}{}}
\newlabel{eq:1}{{1}{2}{Convolutional Auto-encoder}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2} Variational Auto-encoder}{2}{subsubsection.3.1.2}}
\newlabel{sec:vari-auto-encod}{{3.1.2}{2}{Variational Auto-encoder}{subsubsection.3.1.2}{}}
\newlabel{eq:3}{{2}{2}{Variational Auto-encoder}{equation.3.2}{}}
\newlabel{eq:2}{{3}{2}{Variational Auto-encoder}{equation.3.3}{}}
\newlabel{eq:4}{{4}{2}{Variational Auto-encoder}{equation.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3} ``t-SNE Style'' Auto-encoder}{2}{subsubsection.3.1.3}}
\newlabel{sec:t-sne-style}{{3.1.3}{2}{``t-SNE Style'' Auto-encoder}{subsubsection.3.1.3}{}}
\newlabel{eq:5}{{5}{2}{``t-SNE Style'' Auto-encoder}{equation.3.5}{}}
\newlabel{eq:7}{{6}{2}{``t-SNE Style'' Auto-encoder}{equation.3.6}{}}
\citation{johnson_hierarchical_1967}
\citation{lecun_mnist_2010}
\newlabel{eq:6}{{7}{3}{``t-SNE Style'' Auto-encoder}{equation.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2} Sampling Strategies}{3}{subsection.3.2}}
\newlabel{sec:sampling-strategies}{{3.2}{3}{Sampling Strategies}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4} Results}{3}{section.4}}
\newlabel{sec:results}{{4}{3}{Results}{section.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Select examples from the clustered encoding $Z$ according to a distribution $p_S$ on each cluster $S$. Applies to sampling strategies without clustering if $\mathbf  {S} = \{[N]\}$.}}{3}{algorithm.1}}
\newlabel{alg:sampling}{{1}{3}{Sampling Strategies}{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5} Discussion}{3}{section.5}}
\newlabel{sec:discussion}{{5}{3}{Discussion}{section.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Test-set accuracies (\%) for a classifier trained on query sets from both the balanced and unbalanced MNIST data. As can be seen, the advantage of latent space sampling is most apparent when the original data is unbalanced and the sample size is small enough to reflect it.}}{4}{table.1}}
\newlabel{tab:results}{{1}{4}{Test-set accuracies (\%) for a classifier trained on query sets from both the balanced and unbalanced MNIST data. As can be seen, the advantage of latent space sampling is most apparent when the original data is unbalanced and the sample size is small enough to reflect it}{table.1}{}}
\newlabel{fig:vae-encodings}{{2a}{4}{Subfigure 2a}{subfigure.2.1}{}}
\newlabel{sub@fig:vae-encodings}{{(a)}{a}{Subfigure 2a\relax }{subfigure.2.1}{}}
\newlabel{fig:classifier-accs}{{2b}{4}{Subfigure 2b}{subfigure.2.2}{}}
\newlabel{sub@fig:classifier-accs}{{(b)}{b}{Subfigure 2b\relax }{subfigure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (\ref  {fig:vae-encodings}) shows latent-space encoding for the unbalanced MNIST training set using a VAE. (\ref  {fig:classifier-losses}) shows the validation set loss during training for a classifier on a training set of size 100. (Epoch count in 10s.)}}{4}{figure.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{4}{subfigure.2.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{4}{subfigure.2.2}}
\newlabel{fig:unbalanced-mnist-results}{{2}{4}{(\ref {fig:vae-encodings}) shows latent-space encoding for the unbalanced MNIST training set using a VAE. (\ref {fig:classifier-losses}) shows the validation set loss during training for a classifier on a training set of size 100. (Epoch count in 10s.)}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6} Conclusions}{4}{section.6}}
\newlabel{sec:conclusion}{{6}{4}{Conclusions}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7} Acknowledgements}{4}{section.7}}
\newlabel{sec:acknowledgements}{{7}{4}{Acknowledgements}{section.7}{}}
\bibstyle{IEEEtran}
\bibdata{report}
\bibcite{settles_active_2012}{1}
\bibcite{lewis_sequential_1994}{2}
\bibcite{zhu_semi-supervised_2005}{3}
\bibcite{goodfellow_deep_nodate}{4}
\bibcite{krizhevsky_imagenet_2012}{5}
\bibcite{nair_rectified_nodate}{6}
\bibcite{kingma_auto-encoding_2013}{7}
\bibcite{maaten_learning_2009}{8}
\bibcite{johnson_hierarchical_1967}{9}
\bibcite{lecun_mnist_2010}{10}
\@writefile{toc}{\contentsline {section}{\numberline {8} References}{5}{section.8}}
