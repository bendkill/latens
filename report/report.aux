\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{nair_rectified_nodate}
\@writefile{toc}{\contentsline {section}{\numberline {1} Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2} Method}{1}{section.2}}
\newlabel{sec:method}{{2}{1}{Method}{section.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Approximate a uniform sampling of the latent-space hypercube $[0,1]^L$ from the encoding $Z$.}}{1}{algorithm.1}}
\newlabel{alg:uniform-sampling}{{1}{1}{Method}{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3} Results}{1}{section.3}}
\newlabel{sec:results}{{3}{1}{Results}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4} Discussion}{1}{section.4}}
\newlabel{sec:discussion}{{4}{1}{Discussion}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces an overview of the query selection process. Given a large, unlabeled dataset, we train an autoencoder to generate latent space representations of each example. Next, the ``latent space sampling'' identifies examples which are well-spread-out in the low-dimensional encoding. Finally, the selected examples in the original dataset are labeled for training.}}{2}{figure.1}}
\newlabel{fig:overview}{{1}{2}{an overview of the query selection process. Given a large, unlabeled dataset, we train an autoencoder to generate latent space representations of each example. Next, the ``latent space sampling'' identifies examples which are well-spread-out in the low-dimensional encoding. Finally, the selected examples in the original dataset are labeled for training}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces a visualization of the latent space $[0,1]^{10}$ learned by a simple convolutional autoencoder. The $i$th row corresponds to variation of the $i$th latent dimension from $0$ to $1$, fixing all other coordinates at $0.3$. So the 9th image in the 3rd row represents the decoding of $[0.3,0.3, 0.45, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]$.}}{2}{figure.2}}
\newlabel{fig:autoencoder-visualization}{{2}{2}{a visualization of the latent space $[0,1]^{10}$ learned by a simple convolutional autoencoder. The $i$th row corresponds to variation of the $i$th latent dimension from $0$ to $1$, fixing all other coordinates at $0.3$. So the 9th image in the 3rd row represents the decoding of $[0.3,0.3, 0.45, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]$}{figure.2}{}}
\bibstyle{IEEEtran}
\bibdata{report}
\bibcite{nair_rectified_nodate}{1}
\@writefile{toc}{\contentsline {section}{\numberline {5} References}{3}{section.5}}
