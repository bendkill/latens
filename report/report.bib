
@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2018-06-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105},
	file = {NIPS Full Text PDF:/Users/Benjamin/Zotero/storage/DETBKZHI/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf;NIPS Snapshort:/Users/Benjamin/Zotero/storage/P7Z8NW3U/4824-imagenet-classification-with-deep-convolutional-neural-networks.html:text/html}
}

@article{lin_microsoft_2014,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2018-06-01},
	journal = {arXiv:1405.0312 [cs]},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = may,
	year = {2014},
	note = {arXiv: 1405.0312},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1405.0312 PDF:/Users/Benjamin/Zotero/storage/B2KZPIIN/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/D3N5TVNA/1405.html:text/html}
}

@article{nash_topological_2015,
	title = {Topological mechanics of gyroscopic metamaterials},
	volume = {112},
	issn = {0027-8424},
	url = {http://www.pnas.org/content/112/47/14495},
	doi = {10.1073/pnas.1507413112},
	abstract = {We have built a new type of mechanical metamaterial: a “gyroscopic metamaterial” composed of rapidly spinning objects that are coupled to each other. At the edges of these materials, we find sound waves that are topologically protected (i.e. they cannot be scattered backward or into the bulk). These waves, which propagate in one direction only, are directly analogous to edge currents in quantum Hall systems. Through a mathematical model, we interpret the robustness of these edge waves in light of the subtle topological character of the bulk material. Crucially, these edge motions can be controlled by distorting the metamaterial lattice, opening new avenues for the control of sound in matter.Topological mechanical metamaterials are artificial structures whose unusual properties are protected very much like their electronic and optical counterparts. Here, we present an experimental and theoretical study of an active metamaterial—composed of coupled gyroscopes on a lattice—that breaks time-reversal symmetry. The vibrational spectrum displays a sonic gap populated by topologically protected edge modes that propagate in only one direction and are unaffected by disorder. We present a mathematical model that explains how the edge mode chirality can be switched via controlled distortions of the underlying lattice. This effect allows the direction of the edge current to be determined on demand. We demonstrate this functionality in experiment and envision applications of these edge modes to the design of one-way acoustic waveguides.},
	number = {47},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nash, Lisa M. and Kleckner, Dustin and Read, Alismari and Vitelli, Vincenzo and Turner, Ari M. and Irvine, William T. M.},
	year = {2015},
	pages = {14495--14500}
}

@article{bai_deep_2016,
	title = {Deep {Watershed} {Transform} for {Instance} {Segmentation}},
	url = {http://arxiv.org/abs/1611.08303},
	abstract = {Most contemporary approaches to instance segmentation use complex pipelines involving conditional random fields, recurrent neural networks, object proposals, or template matching schemes. In our paper, we present a simple yet powerful end-to-end convolutional neural network to tackle this task. Our approach combines intuitions from the classical watershed transform and modern deep learning to produce an energy map of the image where object instances are unambiguously represented as basins in the energy map. We then perform a cut at a single energy level to directly yield connected components corresponding to object instances. Our model more than doubles the performance of the state-of-the-art on the challenging Cityscapes Instance Level Segmentation task.},
	urldate = {2018-10-19},
	journal = {arXiv:1611.08303 [cs]},
	author = {Bai, Min and Urtasun, Raquel},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.08303},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1611.08303 PDF:/Users/Benjamin/Zotero/storage/WKPTMBWF/Bai and Urtasun - 2016 - Deep Watershed Transform for Instance Segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/5JQQETTQ/1611.html:text/html}
}

@book{barber_bayesian_2011,
	address = {Cambridge},
	title = {Bayesian {Reasoning} and {Machine} {Learning}},
	isbn = {978-0-511-80477-9},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511804779},
	language = {en},
	urldate = {2018-10-19},
	publisher = {Cambridge University Press},
	author = {Barber, David},
	year = {2011},
	doi = {10.1017/CBO9780511804779},
	file = {Barber - 2011 - Bayesian Reasoning and Machine Learning.pdf:/Users/Benjamin/Zotero/storage/Z9P4DAL4/Barber - 2011 - Bayesian Reasoning and Machine Learning.pdf:application/pdf}
}

@article{long_fully_nodate,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efﬁcient inference and learning. We deﬁne and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classiﬁcation networks (AlexNet [22], the VGG net [34], and GoogLeNet [35]) into fully convolutional networks and transfer their learned representations by ﬁne-tuning [5] to the segmentation task. We then deﬁne a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, ﬁne layer to produce accurate and detailed segmentations. Our fully convolutional network achieves stateof-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one ﬁfth of a second for a typical image.},
	language = {en},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	pages = {10},
	file = {Long et al. - Fully Convolutional Networks for Semantic Segmenta.pdf:/Users/Benjamin/Zotero/storage/3N9YM7G6/Long et al. - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf}
}

@article{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2018-10-19},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1506.01497 PDF:/Users/Benjamin/Zotero/storage/8HTDVZCT/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/JE3KGFZT/1506.html:text/html}
}

@article{kao_localization-aware_2018,
	title = {Localization-{Aware} {Active} {Learning} for {Object} {Detection}},
	url = {http://arxiv.org/abs/1801.05124},
	abstract = {Active learning - a class of algorithms that iteratively searches for the most informative samples to include in a training dataset - has been shown to be effective at annotating data for image classification. However, the use of active learning for object detection is still largely unexplored as determining informativeness of an object-location hypothesis is more difficult. In this paper, we address this issue and present two metrics for measuring the informativeness of an object hypothesis, which allow us to leverage active learning to reduce the amount of annotated data needed to achieve a target object detection performance. Our first metric measures 'localization tightness' of an object hypothesis, which is based on the overlapping ratio between the region proposal and the final prediction. Our second metric measures 'localization stability' of an object hypothesis, which is based on the variation of predicted object locations when input images are corrupted by noise. Our experimental results show that by augmenting a conventional active-learning algorithm designed for classification with the proposed metrics, the amount of labeled training data required can be reduced up to 25\%. Moreover, on PASCAL 2007 and 2012 datasets our localization-stability method has an average relative improvement of 96.5\% and 81.9\% over the baseline method using classification only.},
	urldate = {2018-10-19},
	journal = {arXiv:1801.05124 [cs]},
	author = {Kao, Chieh-Chi and Lee, Teng-Yok and Sen, Pradeep and Liu, Ming-Yu},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.05124},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1801.05124 PDF:/Users/Benjamin/Zotero/storage/8LRSSP5N/Kao et al. - 2018 - Localization-Aware Active Learning for Object Dete.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/A2WGGXZP/1801.html:text/html}
}

@inproceedings{jain_active_2016,
	title = {Active {Image} {Segmentation} {Propagation}},
	doi = {10.1109/CVPR.2016.313},
	abstract = {We propose a semi-automatic method to obtain foreground object masks for a large set of related images. We develop a stagewise active approach to propagation: in each stage, we actively determine the images that appear most valuable for human annotation, then revise the foreground estimates in all unlabeled images accordingly. In order to identify images that, once annotated, will propagate well to other examples, we introduce an active selection procedure that operates on the joint segmentation graph over all images. It prioritizes human intervention for those images that are uncertain and influential in the graph, while also mutually diverse. We apply our method to obtain foreground masks for over 1 million images. Our method yields state-of-the-art accuracy on the ImageNet and MIT Object Discovery datasets, and it focuses human attention more effectively than existing propagation strategies.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Jain, S. D. and Grauman, K.},
	month = jun,
	year = {2016},
	keywords = {active image segmentation propagation, active selection, Computer vision, Feature extraction, feature selection, Focusing, foreground estimates, foreground object masks, graph theory, human annotation, image annotation, image segmentation, Image segmentation, ImageNet datasets, Manuals, MIT object discovery datasets, Proposals, segmentation graph, Spatial databases, stagewise active approach, unlabeled images},
	pages = {2864--2873},
	file = {IEEE Xplore Abstract Record:/Users/Benjamin/Zotero/storage/VWBWLIDT/7780682.html:text/html;IEEE Xplore Full Text PDF:/Users/Benjamin/Zotero/storage/BLXEQQIC/Jain and Grauman - 2016 - Active Image Segmentation Propagation.pdf:application/pdf}
}

@inproceedings{vezhnevets_active_2012,
	title = {Active learning for semantic segmentation with expected change},
	doi = {10.1109/CVPR.2012.6248050},
	abstract = {We address the problem of semantic segmentation: classifying each pixel in an image according to the semantic class it belongs to (e.g. dog, road, car). Most existing methods train from fully supervised images, where each pixel is annotated by a class label. To reduce the annotation effort, recently a few weakly supervised approaches emerged. These require only image labels indicating which classes are present. Although their performance reaches a satisfactory level, there is still a substantial gap between the accuracy of fully and weakly supervised methods. We address this gap with a novel active learning method specifically suited for this setting. We model the problem as a pairwise CRF and cast active learning as finding its most informative nodes. These nodes induce the largest expected change in the overall CRF state, after revealing their true label. Our criterion is equivalent to maximizing an upper-bound on accuracy gain. Experiments on two data-sets show that our method achieves 97\% percent of the accuracy of the corresponding fully supervised model, while querying less than 17\% of the (super-)pixel labels.},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Vezhnevets, A. and Buhmann, J. M. and Ferrari, V.},
	month = jun,
	year = {2012},
	keywords = {image segmentation, Image segmentation, Accuracy, active learning, class label, Computational modeling, expected change, fully supervised images, image label, Labeling, learning (artificial intelligence), pairwise CRF, Roads, semantic segmentation, Semantics, Training, weakly supervised method},
	pages = {3162--3169},
	file = {IEEE Xplore Abstract Record:/Users/Benjamin/Zotero/storage/FDJACNQY/6248050.html:text/html;IEEE Xplore Full Text PDF:/Users/Benjamin/Zotero/storage/3U9YSYWM/Vezhnevets et al. - 2012 - Active learning for semantic segmentation with exp.pdf:application/pdf}
}

@article{akeret_radio_2016,
	title = {Radio frequency interference mitigation using deep convolutional neural networks},
	url = {http://arxiv.org/abs/1609.09077},
	abstract = {We propose a novel approach for mitigating radio frequency interference (RFI) signals in radio data using the latest advances in deep learning. We employ a special type of Convolutional Neural Network, the U-Net, that enables the classification of clean signal and RFI signatures in 2D time-ordered data acquired from a radio telescope. We train and assess the performance of this network using the HIDE \& SEEK radio data simulation and processing packages, as well as early Science Verification data acquired with the 7m single-dish telescope at the Bleien Observatory. We find that our U-Net implementation is showing competitive accuracy to classical RFI mitigation algorithms such as SEEK's SumThreshold implementation. We publish our U-Net software package on GitHub under GPLv3 license.},
	urldate = {2018-10-21},
	journal = {arXiv:1609.09077 [astro-ph]},
	author = {Akeret, Joel and Chang, Chihway and Lucchi, Aurelien and Refregier, Alexandre},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.09077},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
	file = {arXiv\:1609.09077 PDF:/Users/Benjamin/Zotero/storage/BFXCP5RT/Akeret et al. - 2016 - Radio frequency interference mitigation using deep.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/TEEHFZDX/1609.html:text/html}
}

@article{chen_encoder-decoder_2018,
	title = {Encoder-{Decoder} with {Atrous} {Separable} {Convolution} for {Semantic} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1802.02611},
	abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0{\textbackslash}\% and 82.1{\textbackslash}\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at {\textbackslash}url\{https://github.com/tensorflow/models/tree/master/research/deeplab\}.},
	urldate = {2018-10-21},
	journal = {arXiv:1802.02611 [cs]},
	author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.02611},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1802.02611 PDF:/Users/Benjamin/Zotero/storage/LAGWSPGK/Chen et al. - 2018 - Encoder-Decoder with Atrous Separable Convolution .pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/IDT623VC/1802.html:text/html}
}

@article{chen_semantic_2014,
	title = {Semantic {Image} {Segmentation} with {Deep} {Convolutional} {Nets} and {Fully} {Connected} {CRFs}},
	url = {http://arxiv.org/abs/1412.7062},
	abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6\% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
	urldate = {2018-10-21},
	journal = {arXiv:1412.7062 [cs]},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.7062},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1412.7062 PDF:/Users/Benjamin/Zotero/storage/74XSH9GU/Chen et al. - 2014 - Semantic Image Segmentation with Deep Convolutiona.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/4GUXZ4BR/1412.html:text/html}
}

@article{liu_parsenet:_2015,
	title = {{ParseNet}: {Looking} {Wider} to {See} {Better}},
	shorttitle = {{ParseNet}},
	url = {http://arxiv.org/abs/1506.04579},
	abstract = {We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at https://github.com/weiliu89/caffe/tree/fcn .},
	urldate = {2018-10-21},
	journal = {arXiv:1506.04579 [cs]},
	author = {Liu, Wei and Rabinovich, Andrew and Berg, Alexander C.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.04579},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1506.04579 PDF:/Users/Benjamin/Zotero/storage/7CS94QHF/Liu et al. - 2015 - ParseNet Looking Wider to See Better.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/ZB58U7YZ/1506.html:text/html}
}

@inproceedings{bernardis_finding_2010,
	title = {Finding dots: {Segmentation} as popping out regions from boundaries},
	shorttitle = {Finding dots},
	doi = {10.1109/CVPR.2010.5540210},
	abstract = {Many applications need to segment out all small round regions in an image. This task of finding dots can be viewed as a region segmentation problem where the dots form one region and the areas between dots form the other. We formulate it as a graph cuts problem with two types of grouping cues: short-range attraction based on feature similarity and long-range repulsion based on feature dissimilarity. The feature we use is a pixel-centric relational representation that encodes local convexity: Pixels inside the dots and outside the dots become sinks and sources of the feature vector. Normalized cuts on both attraction and repulsion pop out all the dots in a single binary segmentation. Our experiments show that our method is more accurate and robust than state-of-art segmentation algorithms on four categories of microscopic images. It can also detect textons in natural scene images with the same set of parameters.},
	booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Bernardis, E. and Yu, S. X.},
	month = jun,
	year = {2010},
	keywords = {Layout, graph theory, image segmentation, Image segmentation, Auditory system, Biomedical imaging, Cancer, Educational institutions, Embryo, feature dissimilarity, feature similarity, finding dots, graph cuts problem, image representation, image resolution, long-range repulsion, Microscopy, object detection, pixel-centric relational representation, popping out regions, region segmentation problem, Robustness, short-range attraction, Silicon, single binary segmentation},
	pages = {199--206},
	file = {IEEE Xplore Abstract Record:/Users/Benjamin/Zotero/storage/L9MVBJ3E/5540210.html:text/html;IEEE Xplore Full Text PDF:/Users/Benjamin/Zotero/storage/EBB8XA5V/Bernardis and Yu - 2010 - Finding dots Segmentation as popping out regions .pdf:application/pdf}
}

@inproceedings{maire_affinity_2016,
	title = {Affinity {CNN}: {Learning} {Pixel}-{Centric} {Pairwise} {Relations} for {Figure}/{Ground} {Embedding}},
	shorttitle = {Affinity {CNN}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Maire_Affinity_CNN_Learning_CVPR_2016_paper.html},
	urldate = {2018-11-06},
	author = {Maire, Michael and Narihira, Takuya and Yu, Stella X.},
	year = {2016},
	pages = {174--182},
	file = {Full Text PDF:/Users/Benjamin/Zotero/storage/SWTDPNL7/Maire et al. - 2016 - Affinity CNN Learning Pixel-Centric Pairwise Rela.pdf:application/pdf;Snapshot:/Users/Benjamin/Zotero/storage/USK9EBMV/Maire_Affinity_CNN_Learning_CVPR_2016_paper.html:text/html}
}

@article{zhao_pyramid_2016,
	title = {Pyramid {Scene} {Parsing} {Network}},
	url = {http://arxiv.org/abs/1612.01105},
	abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4\% on PASCAL VOC 2012 and accuracy 80.2\% on Cityscapes.},
	urldate = {2018-11-06},
	journal = {arXiv:1612.01105 [cs]},
	author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.01105},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1612.01105 PDF:/Users/Benjamin/Zotero/storage/EAQ6VLBR/Zhao et al. - 2016 - Pyramid Scene Parsing Network.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/YP7AKCGM/1612.html:text/html}
}

@article{ronneberger_u-net:_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2018-11-08},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1505.04597 PDF:/Users/Benjamin/Zotero/storage/6R2U6YQ4/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/RHCR3G5P/1505.html:text/html}
}

@article{deng_imagenet:_nodate,
	title = {{ImageNet}: {A} {Large}-{Scale} {Hierarchical} {Image} {Database}},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	language = {en},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	pages = {8},
	file = {Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf:/Users/Benjamin/Zotero/storage/FLRF5VCQ/Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf:application/pdf}
}

@inproceedings{pathak_context_2016,
	address = {Las Vegas, NV, USA},
	title = {Context {Encoders}: {Feature} {Learning} by {Inpainting}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {Context {Encoders}},
	url = {http://ieeexplore.ieee.org/document/7780647/},
	doi = {10.1109/CVPR.2016.278},
	abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders – a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classiﬁcation, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
	language = {en},
	urldate = {2018-11-26},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
	month = jun,
	year = {2016},
	pages = {2536--2544},
	file = {Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf:/Users/Benjamin/Zotero/storage/W24KWXYG/Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf:application/pdf}
}

@incollection{hutchison_undoing_2012,
	address = {Berlin, Heidelberg},
	title = {Undoing the {Damage} of {Dataset} {Bias}},
	volume = {7572},
	isbn = {978-3-642-33717-8 978-3-642-33718-5},
	url = {http://link.springer.com/10.1007/978-3-642-33718-5_12},
	language = {en},
	urldate = {2018-11-27},
	booktitle = {Computer {Vision} – {ECCV} 2012},
	publisher = {Springer Berlin Heidelberg},
	author = {Khosla, Aditya and Zhou, Tinghui and Malisiewicz, Tomasz and Efros, Alexei A. and Torralba, Antonio},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
	year = {2012},
	doi = {10.1007/978-3-642-33718-5_12},
	pages = {158--171},
	file = {Khosla et al. - 2012 - Undoing the Damage of Dataset Bias.pdf:/Users/Benjamin/Zotero/storage/HX54S2M9/Khosla et al. - 2012 - Undoing the Damage of Dataset Bias.pdf:application/pdf}
}

@inproceedings{torralba_unbiased_2011,
	title = {Unbiased look at dataset bias},
	doi = {10.1109/CVPR.2011.5995347},
	abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.},
	booktitle = {{CVPR} 2011},
	author = {Torralba, A. and Efros, A. A.},
	month = jun,
	year = {2011},
	keywords = {Training, algorithm evaluation protocols, closed world assumption effects, Communities, contemporary object recognition, cross dataset generalization, data capture, Internet, object recognition, Object recognition, recognition datasets, relative data bias, sample value, Support vector machines, Testing, visual databases, Visualization},
	pages = {1521--1528},
	file = {IEEE Xplore Abstract Record:/Users/Benjamin/Zotero/storage/NGHZC3TP/5995347.html:text/html;IEEE Xplore Full Text PDF:/Users/Benjamin/Zotero/storage/YPJ4ILEN/Torralba and Efros - 2011 - Unbiased look at dataset bias.pdf:application/pdf}
}

@article{settles_active_2012,
	title = {Active {Learning}},
	volume = {6},
	issn = {1939-4608},
	url = {https://www.morganclaypool.com/doi/abs/10.2200/S00429ED1V01Y201207AIM018},
	doi = {10.2200/S00429ED1V01Y201207AIM018},
	number = {1},
	urldate = {2018-11-27},
	journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	author = {Settles, Burr},
	month = jun,
	year = {2012},
	pages = {1--114},
	file = {Settles - Active Learning Literature Survey.pdf:/Users/Benjamin/Zotero/storage/8XM5T954/Settles - Active Learning Literature Survey.pdf:application/pdf;Snapshot:/Users/Benjamin/Zotero/storage/M4QVEJNV/S00429ED1V01Y201207AIM018.html:text/html}
}

@inproceedings{bertalmio_image_2000,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '00},
	title = {Image {Inpainting}},
	isbn = {978-1-58113-208-3},
	url = {http://dx.doi.org/10.1145/344779.344972},
	doi = {10.1145/344779.344972},
	abstract = {Inpainting, the technique of modifying an image in an undetectable form, is as ancient as art itself. The goals and applications of inpainting are numerous, from the restoration of damaged paintings and photographs to the removal/replacement of selected objects. In this paper, we introduce a novel algorithm for digital inpainting of still images that attempts to replicate the basic techniques used by professional restorators. After the user selects the regions to be restored, the algorithm automatically fills-in these regions with information surrounding them. The fill-in is done in such a way that isophote lines arriving at the regions' boundaries are completed inside. In contrast with previous approaches, the technique here introduced does not require the user to specify where the novel information comes from. This is automatically done (and in a fast way), thereby allowing to simultaneously fill-in numerous regions containing completely different structures and surrounding backgrounds. In addition, no limitations are imposed on the topology of the region to be inpainted. Applications of this technique include the restoration of old photographs and damaged film; removal of superimposed text like dates, subtitles, or publicity; and the removal of entire objects from the image like microphones or wires in special effects.},
	urldate = {2018-12-10},
	booktitle = {Proceedings of the 27th {Annual} {Conference} on {Computer} {Graphics} and {Interactive} {Techniques}},
	publisher = {ACM Press/Addison-Wesley Publishing Co.},
	author = {Bertalmio, Marcelo and Sapiro, Guillermo and Caselles, Vincent and Ballester, Coloma},
	year = {2000},
	keywords = {anisotropic diffusion, image restoration, inpainting, isophotes},
	pages = {417--424},
	file = {ACM Full Text PDF:/Users/Benjamin/Zotero/storage/VBSS7JGH/Bertalmio et al. - 2000 - Image Inpainting.pdf:application/pdf}
}

@article{simonyan_very_2014,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2018-12-17},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1409.1556 PDF:/Users/Benjamin/Zotero/storage/7G3BG872/Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/CL6BXQY6/1409.html:text/html}
}

@article{szegedy_going_2014,
	title = {Going {Deeper} with {Convolutions}},
	url = {http://arxiv.org/abs/1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	urldate = {2018-12-17},
	journal = {arXiv:1409.4842 [cs]},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.4842},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1409.4842 PDF:/Users/Benjamin/Zotero/storage/Q8C37FGK/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/I4A5GI7P/1409.html:text/html}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2018-12-17},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1512.03385 PDF:/Users/Benjamin/Zotero/storage/VI3WAY3Q/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/IAXDV39B/1512.html:text/html}
}

@inproceedings{long_fully_2015,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html},
	urldate = {2018-12-17},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	year = {2015},
	pages = {3431--3440},
	file = {Full Text PDF:/Users/Benjamin/Zotero/storage/6X6MG622/Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf;Snapshot:/Users/Benjamin/Zotero/storage/PQRZEN76/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html:text/html}
}

@misc{duquevieira_simple_2012,
	title = {Simple basket ball fall},
	shorttitle = {English},
	url = {https://commons.wikimedia.org/wiki/File:Fall.png},
	urldate = {2018-12-17},
	author = {{Duquevieira}},
	month = jan,
	year = {2012},
	file = {Wikimedia Snapshot:/Users/Benjamin/Zotero/storage/NCPAGQRM/FileFall.html:text/html}
}

@misc{munroe_wikipedian_nodate,
	title = {Wikipedian {Protester}},
	url = {https://xkcd.com/285/},
	urldate = {2018-12-17},
	journal = {xkcd},
	author = {Munroe, Randall},
	file = {xkcd\: Wikipedian Protester:/Users/Benjamin/Zotero/storage/NZBYJNE2/285.html:text/html}
}

@incollection{dosovitskiy_discriminative_2014,
	title = {Discriminative {Unsupervised} {Feature} {Learning} with {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.pdf},
	urldate = {2019-01-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Riedmiller, Martin and Brox, Thomas},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {766--774},
	file = {NIPS Full Text PDF:/Users/Benjamin/Zotero/storage/IEZQ62G8/Dosovitskiy et al. - 2014 - Discriminative Unsupervised Feature Learning with .pdf:application/pdf;NIPS Snapshot:/Users/Benjamin/Zotero/storage/6UVW65NU/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.html:text/html}
}

@article{maia_multivariate_nodate,
	title = {Multivariate {Empirical} {Cumulative} {Distribution} {Functions}},
	language = {en},
	author = {Maia, Charlotte},
	pages = {11},
	file = {Maia - Multivariate Empirical Cumulative Distribution Fun.pdf:/Users/Benjamin/Zotero/storage/MEKUS9YQ/Maia - Multivariate Empirical Cumulative Distribution Fun.pdf:application/pdf}
}

@article{justel_multivariate_1997,
	title = {A multivariate {Kolmogorov}- {Smirnov} test of goodness of fit},
	abstract = {Abstract \_ This paper presents a distribution free multivariate Kolmogorov-Smirnov good- ness of fit test. The test uses an statistic which is built using Rosenblatt's transformation and an algorithm is developed to compute it in the bivariate case. An approximate test, that can be easily computed in any dimension, is also presented. The power of these multivariate tests is studied in a simulation study. Key words: Empirical distribution function. Kolmogorov-Smirnov statistics. Rosenblatt's transformation.},
	journal = {Statist, Probab. Lett},
	author = {Justel, Ana and Pefia, Daniel and Zamar, Ruben and Statistics, Departament Of},
	year = {1997},
	pages = {259},
	file = {Citeseer - Full Text PDF:/Users/Benjamin/Zotero/storage/YXC4BMXI/Justel et al. - 1997 - A multivariate Kolmogorov- Smirnov test of goodnes.pdf:application/pdf;Citeseer - Snapshot:/Users/Benjamin/Zotero/storage/GVCDCYG2/summary.html:text/html}
}

@misc{goodfellow_deep_nodate,
	title = {Deep {Learning}},
	url = {http://www.deeplearningbook.org/contents/autoencoders.html},
	urldate = {2019-03-07},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	file = {:/Users/Benjamin/Zotero/storage/N5TC9EC3/autoencoders.html:text/html}
}

@article{nair_rectified_nodate,
	title = {Rectified {Linear} {Units} {Improve} {Restricted} {Boltzmann} {Machines}},
	abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
	language = {en},
	author = {Nair, Vinod and Hinton, Geoffrey E},
	pages = {8},
	file = {Nair and Hinton - Rectified Linear Units Improve Restricted Boltzman.pdf:/Users/Benjamin/Zotero/storage/XNC784S8/Nair and Hinton - Rectified Linear Units Improve Restricted Boltzman.pdf:application/pdf}
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick and {others}},
	year = {1998},
	pages = {2278--2324}
}

@article{kingma_auto-encoding_2013,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2019-03-13},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1312.6114 PDF:/Users/Benjamin/Zotero/storage/6WRLI9P7/Kingma and Welling - 2013 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/Benjamin/Zotero/storage/A5PPVL5H/1312.html:text/html}
}

@inproceedings{maaten_learning_2009,
	title = {Learning a parametric embedding by preserving local structure},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Maaten, Laurens},
	year = {2009},
	pages = {384--391}
}

@techreport{zhu_semi-supervised_2005,
	type = {Technical {Report}},
	title = {Semi-{Supervised} {Learning} {Literature} {Survey}},
	url = {https://minds.wisconsin.edu/handle/1793/60444},
	abstract = {We review some of the literature on semi-supervised learning in this paper. Traditional classifiers need labeled data (feature / label pairs) to train. Labeled instances however are often difficult, expensive, or time consuming to obtain, as they require the efforts of experienced human annotators. Meanwhile unlabeled data may be relatively easy to collect, but there has been few ways to use them. Semi-supervised learning addresses this problem by using large amount of unlabeled data, together with the labeled data, to build better classifiers. Because semi-supervised learning requires less human effort and gives higher 
accuracy, it is of great interest both in theory and in practice.},
	language = {en},
	urldate = {2019-03-16},
	institution = {University of Wisconsin-Madison Department of Computer Sciences},
	author = {Zhu, Xiaojin (Jerry)},
	year = {2005},
	file = {Full Text PDF:/Users/Benjamin/Zotero/storage/XLL2EZXF/Zhu - 2005 - Semi-Supervised Learning Literature Survey.pdf:application/pdf;Snapshot:/Users/Benjamin/Zotero/storage/AZ5CIHBX/60444.html:text/html}
}

@inproceedings{lewis_sequential_1994,
	title = {A sequential algorithm for training text classifiers},
	booktitle = {{SIGIR}’94},
	publisher = {Springer},
	author = {Lewis, David D and Gale, William A},
	year = {1994},
	pages = {3--12}
}

@article{lecun_mnist_2010,
	title = {{MNIST} handwritten digit database},
	url = {http://yann.lecun.com/exdb/mnist/},
	urldate = {2016-01-14},
	author = {LeCun, Yann and Cortes, Corinna},
	year = {2010},
	keywords = {MSc \_checked character\_recognition mnist network neural}
}

@article{johnson_hierarchical_1967,
	title = {Hierarchical clustering schemes},
	volume = {32},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02289588},
	doi = {10.1007/BF02289588},
	abstract = {Techniques for partitioning objects into optimally homogeneous groups on the basis of empirical measures of similarity among those objects have received increasing attention in several different fields. This paper develops a useful correspondence between any hierarchical system of such clusters, and a particular type of distance measure. The correspondence gives rise to two methods of clustering that are computationally rapid and invariant under monotonic transformations of the data. In an explicitly defined sense, one method forms clusters that are optimally “connected,” while the other forms clusters that are optimally “compact.”},
	language = {en},
	number = {3},
	urldate = {2019-03-19},
	journal = {Psychometrika},
	author = {Johnson, Stephen C.},
	month = sep,
	year = {1967},
	keywords = {Distance Measure, Hierarchical Cluster, Homogeneous Group, Public Policy, Statistical Theory},
	pages = {241--254},
	file = {Springer Full Text PDF:/Users/Benjamin/Zotero/storage/PRNXFUU5/Johnson - 1967 - Hierarchical clustering schemes.pdf:application/pdf}
}